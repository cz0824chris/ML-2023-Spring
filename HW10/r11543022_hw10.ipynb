{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"H9m2AbpHC9vS"},"source":["# **Homework 10 - Adversarial Attack**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k0G8g5KuDBzU"},"source":["## Enviroment & Download\n","\n","We make use of [pytorchcv](https://pypi.org/project/pytorchcv/) to obtain CIFAR-10 pretrained model, so we need to set up the enviroment first. We also need to download the data (200 images) which we want to attack."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:35:51.854718Z","iopub.status.busy":"2023-05-24T04:35:51.854337Z","iopub.status.idle":"2023-05-24T04:35:52.828125Z","shell.execute_reply":"2023-05-24T04:35:52.827017Z","shell.execute_reply.started":"2023-05-24T04:35:51.854687Z"},"id":"Oh4CcIFIr238","outputId":"377f43ca-9212-43e5-929f-44f2cbd3eebb","trusted":true},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:35:52.832678Z","iopub.status.busy":"2023-05-24T04:35:52.832355Z","iopub.status.idle":"2023-05-24T04:36:19.103590Z","shell.execute_reply":"2023-05-24T04:36:19.102368Z","shell.execute_reply.started":"2023-05-24T04:35:52.832648Z"},"id":"yMK1RhUQCz1e","outputId":"c13dab84-ecdd-4d69-ee8a-25b9a8eb7623","trusted":true},"outputs":[],"source":["# set up environment\n","!pip install pytorchcv\n","!pip install imgaug\n","\n","# download\n","# !gdown --id 1t2UFQXr1cr5qLMBK2oN2rY1NDypi9Nyw --output data.zip\n","\n","# if the above link isn't available, try this one\n","!wget https://www.dropbox.com/s/lbpypqamqjpt2qz/data.zip\n","\n","# unzip\n","!unzip ./data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:19.106915Z","iopub.status.busy":"2023-05-24T04:36:19.105324Z","iopub.status.idle":"2023-05-24T04:36:20.044346Z","shell.execute_reply":"2023-05-24T04:36:20.043021Z","shell.execute_reply.started":"2023-05-24T04:36:19.106876Z"},"id":"-a6naDouEWUZ","trusted":true},"outputs":[],"source":["!rm ./data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:20.049197Z","iopub.status.busy":"2023-05-24T04:36:20.048893Z","iopub.status.idle":"2023-05-24T04:36:23.449147Z","shell.execute_reply":"2023-05-24T04:36:23.448141Z","shell.execute_reply.started":"2023-05-24T04:36:20.049168Z"},"id":"SaEEx0Y3DMdu","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from pytorchcv.model_provider import get_model as ptcv_get_model\n","import random\n","import numpy as np\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","batch_size = 8\n","def same_seeds(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","same_seeds(24)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Z8mIr7c0DPsh"},"source":["## Global Settings \n","#### **[NOTE]**: Don't change the settings here, or your generated image might not meet the constraint.\n","* $\\epsilon$ is fixed to be 8. But on **Data section**, we will first apply transforms on raw pixel value (0-255 scale) **by ToTensor (to 0-1 scale)** and then **Normalize (subtract mean divide std)**. $\\epsilon$ should be set to $\\frac{8}{255 * std}$ during attack.\n","\n","* Explaination (optional)\n","    * Denote the first pixel of original image as $p$, and the first pixel of adversarial image as $a$.\n","    * The $\\epsilon$ constraints tell us $\\left| p-a \\right| <= 8$.\n","    * ToTensor() can be seen as a function where $T(x) = x/255$.\n","    * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n","    * After applying ToTensor() and Normalize() on $p$ and $a$, the constraint becomes $\\left| N(T(p))-N(T(a)) \\right| = \\left| \\frac{\\frac{p}{255}-mean}{std}-\\frac{\\frac{a}{255}-mean}{std} \\right| = \\frac{1}{255 * std} \\left| p-a \\right| <= \\frac{8}{255 * std}.$\n","    * So, we should set $\\epsilon$ to be $\\frac{8}{255 * std}$ after ToTensor() and Normalize()."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:23.452308Z","iopub.status.busy":"2023-05-24T04:36:23.451681Z","iopub.status.idle":"2023-05-24T04:36:26.506302Z","shell.execute_reply":"2023-05-24T04:36:26.505333Z","shell.execute_reply.started":"2023-05-24T04:36:23.452271Z"},"id":"IBdYgS2DDNL5","trusted":true},"outputs":[],"source":["# the mean and std are the calculated statistics from cifar_10 dataset\n","cifar_10_mean = (0.491, 0.482, 0.447) # mean for the three channels of cifar_10 images\n","cifar_10_std = (0.202, 0.199, 0.201) # std for the three channels of cifar_10 images\n","\n","# convert mean and std to 3-dimensional tensors for future operations\n","mean = torch.tensor(cifar_10_mean).to(device).view(3, 1, 1)\n","std = torch.tensor(cifar_10_std).to(device).view(3, 1, 1)\n","\n","epsilon = 8/255/std"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:26.508041Z","iopub.status.busy":"2023-05-24T04:36:26.507666Z","iopub.status.idle":"2023-05-24T04:36:26.513096Z","shell.execute_reply":"2023-05-24T04:36:26.512236Z","shell.execute_reply.started":"2023-05-24T04:36:26.508007Z"},"id":"AjNkQLoaDWba","trusted":true},"outputs":[],"source":["root = './data' # directory for storing benign images\n","# benign images: images which do not contain adversarial perturbations\n","# adversarial images: images which include adversarial perturbations"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sNf-LoODDZXB"},"source":["## Data\n","\n","Construct dataset and dataloader from root directory. Note that we store the filename of each image for future usage."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:26.515242Z","iopub.status.busy":"2023-05-24T04:36:26.514347Z","iopub.status.idle":"2023-05-24T04:36:26.758034Z","shell.execute_reply":"2023-05-24T04:36:26.757095Z","shell.execute_reply.started":"2023-05-24T04:36:26.515207Z"},"id":"lV7rbnD5DarR","outputId":"25f464ee-ad1b-44f4-f670-bbf2a1e1c22e","trusted":true},"outputs":[],"source":["import os\n","import glob\n","import shutil\n","import numpy as np\n","from PIL import Image\n","from torchvision.transforms import transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(cifar_10_mean, cifar_10_std)\n","])\n","\n","class AdvDataset(Dataset):\n","    def __init__(self, data_dir, transform):\n","        self.images = []\n","        self.labels = []\n","        self.names = []\n","        '''\n","        data_dir\n","        ├── class_dir\n","        │   ├── class1.png\n","        │   ├── ...\n","        │   ├── class20.png\n","        '''\n","        for i, class_dir in enumerate(sorted(glob.glob(f'{data_dir}/*'))):\n","            images = sorted(glob.glob(f'{class_dir}/*'))\n","            self.images += images\n","            self.labels += ([i] * len(images))\n","            self.names += [os.path.relpath(imgs, data_dir) for imgs in images]\n","        self.transform = transform\n","    def __getitem__(self, idx):\n","        image = self.transform(Image.open(self.images[idx]))\n","        label = self.labels[idx]\n","        return image, label\n","    def __getname__(self):\n","        return self.names\n","    def __len__(self):\n","        return len(self.images)\n","\n","adv_set = AdvDataset(root, transform=transform)\n","adv_names = adv_set.__getname__()\n","adv_loader = DataLoader(adv_set, batch_size=batch_size, shuffle=False)\n","\n","print(f'number of images = {adv_set.__len__()}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"C9D7eakEDflF"},"source":["## Utils -- Benign Images Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:26.760880Z","iopub.status.busy":"2023-05-24T04:36:26.759428Z","iopub.status.idle":"2023-05-24T04:36:26.767783Z","shell.execute_reply":"2023-05-24T04:36:26.766880Z","shell.execute_reply.started":"2023-05-24T04:36:26.760843Z"},"id":"byE4VH3uDduA","trusted":true},"outputs":[],"source":["# to evaluate the performance of model on benign images\n","def epoch_benign(model, loader, loss_fn):\n","    model.eval()\n","    train_acc, train_loss = 0.0, 0.0\n","    for x, y in loader:\n","        x, y = x.to(device), y.to(device)\n","        yp = model(x)\n","        loss = loss_fn(yp, y)\n","        train_acc += (yp.argmax(dim=1) == y).sum().item()\n","        train_loss += loss.item() * x.shape[0]\n","    return train_acc / len(loader.dataset), train_loss / len(loader.dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"D3L_qtufDk4j"},"source":["## Utils -- Attack Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:31.915381Z","iopub.status.busy":"2023-05-24T04:36:31.914832Z","iopub.status.idle":"2023-05-24T04:36:31.933034Z","shell.execute_reply":"2023-05-24T04:36:31.932105Z","shell.execute_reply.started":"2023-05-24T04:36:31.915345Z"},"id":"odTOhtrtDklT","trusted":true},"outputs":[],"source":["#參考資料:https://blog.csdn.net/iwill323/article/details/128031965\n","\n","# perform fgsm attack\n","def fgsm(model, x, y, loss_fn, epsilon=epsilon):\n","    x_adv = x.detach().clone() # initialize x_adv as original benign image x\n","    x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n","    loss = loss_fn(model(x_adv), y) # calculate loss\n","    loss.backward() # calculate gradient\n","    # fgsm: use gradient ascent on x_adv to maximize loss\n","    grad = x_adv.grad.detach()\n","    x_adv = x_adv + epsilon * grad.sign()\n","    return x_adv\n","\n","# alpha and num_iter can be decided by yourself\n","alpha = 0.8/255/std\n","\n","def ifgsm(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=20):\n","    x_adv = x.detach().clone()\n","    ################ TODO: Medium baseline #######################\n","    # write a loop with num_iter times\n","    for i in range(num_iter):\n","      # TODO: Each iteration, execute fgsm\n","      x_adv = fgsm(model, x_adv, y, loss_fn, alpha)\n","      x_adv = torch.min(torch.max(x_adv, x-epsilon), x+epsilon)\n","\n","    return x_adv\n","\n","def mifgsm(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=20, decay=0.8):\n","    x_adv = x.detach().clone().to(device)\n","    # initialze momentum tensor\n","    momentum = torch.zeros_like(x).detach().to(device)\n","\n","    ################ TODO: Strong baseline ####################\n","    for i in range(num_iter):\n","      # TODO: Refer to the algorithm of MI-FGSM\n","      # Calculate the momentum and update\n","      x_adv = x_adv.detach().clone()\n","      x_adv.requires_grad = True\n","      loss = loss_fn(model(x_adv),y)\n","      loss.backward()\n","      grad = x_adv.grad.detach()\n","      grad = decay * momentum + grad / (grad.abs().sum() + 1e-8)\n","      x_adv = x_adv + alpha * grad.sign()\n","      x_adv = torch.max(torch.min(x_adv,x + epsilon),x - epsilon)\n","    return x_adv\n","\n","def dmi_mifgsm(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=48, decay=0.9, p=0.5):\n","  x_adv = x\n","  momentum = torch.zeros_like(x).detach().to(device)\n","  for i in range(num_iter):\n","    x_adv = x_adv.detach().clone()\n","    x_adv_raw = x_adv.clone()\n","    if torch.rand(1).item() >= p :\n","      rnd = torch.randint(29, 33, (1,)).item()\n","      x_adv=transforms.Resize((rnd, rnd))(x_adv)\n","      left = torch.randint(0, 32 - rnd + 1, (1,)).item()\n","      top = torch.randint(0, 32 - rnd + 1, (1,)).item()\n","      right = 32 - rnd - left\n","      bottom = 32 - rnd - top\n","      x_adv = transforms.Pad([left, top, right, bottom])(x_adv)\n","    x_adv.requires_grad = True \n","    loss = loss_fn(model(x_adv), y)\n","    loss.backward()\n","    grad = x_adv.grad.detach()\n","    grad = decay * momentum + grad/(grad.abs().sum() )\n","    momentum = grad\n","    x_adv = x_adv_raw + alpha * grad.sign()\n","    x_adv = torch.max(torch.min(x_adv, x+epsilon), x-epsilon) # clip new x_adv back to [x-epsilon, xtepsilon]\n","  return x_adv"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0o9ww4s1DrEx"},"source":["## Utils -- Attack\n","* Recall\n","  * ToTensor() can be seen as a function where $T(x) = x/255$.\n","  * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n","\n","* Inverse function\n","  * Inverse Normalize() can be seen as a function where $N^{-1}(x) = x*std+mean$ where $mean$ and $std$ are constants.\n","  * Inverse ToTensor() can be seen as a function where $T^{-1}(x) = x*255$.\n","\n","* Special Noted\n","  * ToTensor() will also convert the image from shape (height, width, channel) to shape (channel, height, width), so we also need to transpose the shape back to original shape.\n","  * Since our dataloader samples a batch of data, what we need here is to transpose **(batch_size, channel, height, width)** back to **(batch_size, height, width, channel)** using np.transpose."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:31.934897Z","iopub.status.busy":"2023-05-24T04:36:31.934449Z","iopub.status.idle":"2023-05-24T04:36:31.952993Z","shell.execute_reply":"2023-05-24T04:36:31.951985Z","shell.execute_reply.started":"2023-05-24T04:36:31.934834Z"},"id":"rbtfv7rjDrvR","trusted":true},"outputs":[],"source":["# perform adversarial attack and generate adversarial examples\n","def gen_adv_examples(model, loader, attack, loss_fn):\n","    model.eval()\n","    adv_names = []\n","    train_acc, train_loss = 0.0, 0.0\n","    for i, (x, y) in enumerate(loader):\n","        x, y = x.to(device), y.to(device)\n","        x_adv = attack(model, x, y, loss_fn) # obtain adversarial examples\n","        yp = model(x_adv)\n","        loss = loss_fn(yp, y)\n","        train_acc += (yp.argmax(dim=1) == y).sum().item()\n","        train_loss += loss.item() * x.shape[0]\n","        # store adversarial examples\n","        adv_ex = ((x_adv) * std + mean).clamp(0, 1) # to 0-1 scale\n","        adv_ex = (adv_ex * 255).clamp(0, 255) # 0-255 scale\n","        adv_ex = adv_ex.detach().cpu().data.numpy().round() # round to remove decimal part\n","        adv_ex = adv_ex.transpose((0, 2, 3, 1)) # transpose (bs, C, H, W) back to (bs, H, W, C)\n","        adv_examples = adv_ex if i == 0 else np.r_[adv_examples, adv_ex]\n","    return adv_examples, train_acc / len(loader.dataset), train_loss / len(loader.dataset)\n","\n","# create directory which stores adversarial examples\n","def create_dir(data_dir, adv_dir, adv_examples, adv_names):\n","    if os.path.exists(adv_dir) is not True:\n","        _ = shutil.copytree(data_dir, adv_dir)\n","    for example, name in zip(adv_examples, adv_names):\n","        im = Image.fromarray(example.astype(np.uint8)) # image pixel value should be unsigned int\n","        im.save(os.path.join(adv_dir, name))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rbLBR4bjDu7h"},"source":["## Model / Loss Function\n","\n","Model list is available [here](https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/model_provider.py). Please select models which has _cifar10 suffix. Other kinds of models are prohibited, and it will be considered to be cheating if you use them. \n","\n","Note: Some of the models cannot be accessed/loaded. You can safely skip them since TA's model will not use those kinds of models."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:31.955836Z","iopub.status.busy":"2023-05-24T04:36:31.955523Z","iopub.status.idle":"2023-05-24T04:36:31.966219Z","shell.execute_reply":"2023-05-24T04:36:31.965302Z","shell.execute_reply.started":"2023-05-24T04:36:31.955809Z"},"id":"xCKMshb08I1I","trusted":true},"outputs":[],"source":["# This function is used to check whether you use models pretrained on cifar10 instead of other datasets\n","def model_checker(model_name):\n","  assert ('cifar10' in model_name) and ('cifar100' not in model_name), 'The model selected is not pretrained on cifar10!'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:31.969493Z","iopub.status.busy":"2023-05-24T04:36:31.968656Z","iopub.status.idle":"2023-05-24T04:36:32.562663Z","shell.execute_reply":"2023-05-24T04:36:32.561028Z","shell.execute_reply.started":"2023-05-24T04:36:31.969459Z"},"id":"eCJU4k__DwPT","outputId":"8f7f728d-977f-482f-86c9-04f42b6d6229","trusted":true},"outputs":[],"source":["from pytorchcv.model_provider import get_model as ptcv_get_model\n","\n","model_name = 'resnet110_cifar10'\n","model_checker(model_name)\n","\n","model = ptcv_get_model(model_name, pretrained=True).to(device)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","benign_acc, benign_loss = epoch_benign(model, adv_loader, loss_fn)\n","print(f'benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-CWEsxsUD0Mo"},"source":["## FGSM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:32.565604Z","iopub.status.busy":"2023-05-24T04:36:32.565248Z","iopub.status.idle":"2023-05-24T04:36:32.569704Z","shell.execute_reply":"2023-05-24T04:36:32.568522Z","shell.execute_reply.started":"2023-05-24T04:36:32.565568Z"},"id":"xP6s-MCODyyh","outputId":"74e10704-c18b-43a5-ca2c-059ffce1b1b6","trusted":true},"outputs":[],"source":["adv_examples, fgsm_acc, fgsm_loss = gen_adv_examples(model, adv_loader, fgsm, loss_fn)\n","print(f'fgsm_acc = {fgsm_acc:.5f}, fgsm_loss = {fgsm_loss:.5f}')\n","\n","create_dir(root, 'fgsm', adv_examples, adv_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:32.571999Z","iopub.status.busy":"2023-05-24T04:36:32.571290Z","iopub.status.idle":"2023-05-24T04:36:32.580039Z","shell.execute_reply":"2023-05-24T04:36:32.579146Z","shell.execute_reply.started":"2023-05-24T04:36:32.571965Z"},"id":"lx-X40vrD3S7","trusted":true},"outputs":[],"source":["# %cd fgsm\n","# !tar zcvf ../fgsm.tgz *\n","# %cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:32.746386Z","iopub.status.busy":"2023-05-24T04:36:32.745488Z","iopub.status.idle":"2023-05-24T04:36:32.750673Z","shell.execute_reply":"2023-05-24T04:36:32.749535Z","shell.execute_reply.started":"2023-05-24T04:36:32.746348Z"},"id":"--9YWbhn_Evr","outputId":"7064e5b1-3c04-42e0-e7f9-25e295dce821","trusted":true},"outputs":[],"source":["# from google.colab import files\n","# files.download('fgsm.tgz')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7dq5LDvJD5rB"},"source":["## Example of Ensemble Attack\n","* Ensemble multiple models as your proxy model to increase the black-box transferability ([paper](https://arxiv.org/abs/1611.02770))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:33.178089Z","iopub.status.busy":"2023-05-24T04:36:33.177368Z","iopub.status.idle":"2023-05-24T04:36:33.184687Z","shell.execute_reply":"2023-05-24T04:36:33.183606Z","shell.execute_reply.started":"2023-05-24T04:36:33.178054Z"},"id":"nvEX_IM8D7Rx","trusted":true},"outputs":[],"source":["################ BOSS BASELINE ######################\n","#參考資料:https://blog.csdn.net/iwill323/article/details/128031965\n","class ensembleNet(nn.Module):\n","    def __init__(self, model_names):\n","        super().__init__()\n","        self.models = nn.ModuleList([ptcv_get_model(name, pretrained=True) for name in model_names])\n","        \n","    def forward(self, x):\n","        #################### TODO: boss baseline ###################\n","        for i, m in enumerate(self.models):\n","          ensemble_logits = m(x) if i==0 else ensemble_logits + m(x)\n","        # TODO: sum up logits from multiple models  \n","        ensemble_logits = ensemble_logits/len(self.models)\n","        return ensemble_logits"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"De5J9n3WD-56"},"source":["* Construct your ensemble model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:33.749523Z","iopub.status.busy":"2023-05-24T04:36:33.748600Z","iopub.status.idle":"2023-05-24T04:36:59.263825Z","shell.execute_reply":"2023-05-24T04:36:59.262688Z","shell.execute_reply.started":"2023-05-24T04:36:33.749489Z"},"id":"9as1WHEiD_cp","outputId":"89029c06-e384-4226-e56f-99bad02fecd5","trusted":true},"outputs":[],"source":["model_names = [\n","    # 'nin_cifar10',\n","    # # 'resnext29_16x64d_cifar10',\n","    # # 'preresnet164bn_cifar10',\n","    # # 'sepreresnet110_cifar10',\n","    # # 'resnet110_cifar10',\n","    # # 'diapreresnet110_cifar10',\n","    # 'resnet20_cifar10',\n","    # 'preresnet20_cifar10',\n","    #*****************************\n","#     'resnext29_16x64d_cifar10',\n","    'resnext29_32x4d_cifar10','preresnet56_cifar10','preresnet110_cifar10',\n","    'preresnet164bn_cifar10','seresnet110_cifar10','sepreresnet56_cifar10','sepreresnet110_cifar10',\n","    'diaresnet56_cifar10','resnet1001_cifar10','diapreresnet56_cifar10','resnet1202_cifar10',\n","    'resnet56_cifar10','resnet110_cifar10','diapreresnet110_cifar10',\n","    'resnext272_1x64d_cifar10','resnext272_2x32d_cifar10'\n","]\n","\n","for model_name in model_names:\n","  model_checker(model_name)\n","\n","ensemble_model = ensembleNet(model_names).to(device)\n","ensemble_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:36:59.265535Z","iopub.status.busy":"2023-05-24T04:36:59.265230Z","iopub.status.idle":"2023-05-24T04:37:16.628864Z","shell.execute_reply":"2023-05-24T04:37:16.627809Z","shell.execute_reply.started":"2023-05-24T04:36:59.265507Z"},"id":"U4dj9bOozhgC","outputId":"790f9451-a646-4f71-e388-e1832807a055","trusted":true},"outputs":[],"source":["from pytorchcv.model_provider import get_model as ptcv_get_model\n","\n","benign_acc, benign_loss = epoch_benign(ensemble_model, adv_loader, loss_fn)\n","print(f'benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6GpnO3kW0hN1"},"source":["# **IFGSM+ensemble**"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-05-24T04:37:16.631511Z","iopub.status.busy":"2023-05-24T04:37:16.630681Z","iopub.status.idle":"2023-05-24T04:37:16.637776Z","shell.execute_reply":"2023-05-24T04:37:16.636811Z","shell.execute_reply.started":"2023-05-24T04:37:16.631471Z"},"id":"WCvUrq_K0giC","outputId":"f73d9bf7-d5a3-4c0f-81be-c6c8f3baeb20","trusted":true},"outputs":[],"source":["# adv_examples, ifgsm_acc, ifgsm_loss = gen_adv_examples(ensemble_model, adv_loader, ifgsm, loss_fn)\n","# print(f'ifgsm_acc = {ifgsm_acc:.5f}, ifgsm_loss = {ifgsm_loss:.5f}')\n","\n","# create_dir(root, 'ifgsm', adv_examples, adv_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:37:16.640849Z","iopub.status.busy":"2023-05-24T04:37:16.640406Z","iopub.status.idle":"2023-05-24T04:37:16.649567Z","shell.execute_reply":"2023-05-24T04:37:16.648536Z","shell.execute_reply.started":"2023-05-24T04:37:16.640816Z"},"id":"ekYQGzQD1KsJ","trusted":true},"outputs":[],"source":["# %cd ifgsm\n","# !tar zcvf ../ifgsm.tgz *\n","# %cd "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SV3bExaq2JbW"},"source":["# **mifgsm+ensemble**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:37:16.651481Z","iopub.status.busy":"2023-05-24T04:37:16.651135Z","iopub.status.idle":"2023-05-24T04:37:16.659820Z","shell.execute_reply":"2023-05-24T04:37:16.658922Z","shell.execute_reply.started":"2023-05-24T04:37:16.651438Z"},"id":"KKqmSFvg3Ofx","trusted":true},"outputs":[],"source":["# adv_examples, mifgsm_acc, mifgsm_loss = gen_adv_examples(ensemble_model, adv_loader, mifgsm, loss_fn)\n","# print(f'mifgsm_acc = {mifgsm_acc:.5f}, mifgsm_loss = {mifgsm_loss:.5f}')\n","\n","# create_dir(root, 'mifgsm', adv_examples, adv_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:37:16.661504Z","iopub.status.busy":"2023-05-24T04:37:16.661064Z","iopub.status.idle":"2023-05-24T04:37:16.669288Z","shell.execute_reply":"2023-05-24T04:37:16.668221Z","shell.execute_reply.started":"2023-05-24T04:37:16.661468Z"},"id":"gz4nJhyO6ona","trusted":true},"outputs":[],"source":["# %cd mifgsm\n","# !tar zcvf ../mifgsm.tgz *\n","# %cd "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:37:16.671051Z","iopub.status.busy":"2023-05-24T04:37:16.670713Z","iopub.status.idle":"2023-05-24T04:37:16.680309Z","shell.execute_reply":"2023-05-24T04:37:16.679337Z","shell.execute_reply.started":"2023-05-24T04:37:16.671018Z"},"id":"CN-kkndA7Ej0","outputId":"69fb84a1-0f13-4669-ce38-f82ac86bcb11","trusted":true},"outputs":[],"source":["# from google.colab import files\n","# files.download('mifgsm.tgz')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jYku_ejk5nkZ"},"source":["# **dim-mifgsm+ensamble**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T04:37:16.682383Z","iopub.status.busy":"2023-05-24T04:37:16.681418Z","iopub.status.idle":"2023-05-24T05:09:10.766291Z","shell.execute_reply":"2023-05-24T05:09:10.765309Z","shell.execute_reply.started":"2023-05-24T04:37:16.682349Z"},"id":"andvWFgZ5mp_","outputId":"3a96010f-3019-4c6e-b6b9-b41025187e4f","trusted":true},"outputs":[],"source":["#13.74743\n","#15.07633\n","#13.50784\n","#13.53069\n","adv_examples, dmi_mifgsm_acc, dmi_mifgsm_loss = gen_adv_examples(ensemble_model, adv_loader, dmi_mifgsm, loss_fn)\n","print(f'dmi_mifgsm_acc = {dmi_mifgsm_acc:.5f}, dmi_mifgsm_loss = {dmi_mifgsm_loss:.5f}')\n","\n","create_dir(root, 'dmi_mifgsm', adv_examples, adv_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:10:33.711774Z","iopub.status.busy":"2023-05-24T05:10:33.711371Z","iopub.status.idle":"2023-05-24T05:10:34.742058Z","shell.execute_reply":"2023-05-24T05:10:34.740736Z","shell.execute_reply.started":"2023-05-24T05:10:33.711742Z"},"id":"lJ-_pTta-MOU","outputId":"8cc74542-69b0-4bd2-e1d5-68db3aec5f2f","trusted":true},"outputs":[],"source":["%cd dmi_mifgsm\n","!tar zcvf ../dmi_mifgsm.tgz *\n","%cd "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GiGKBr5t7Wdk","outputId":"3a2c1687-d82f-4aac-e7a0-d061248ae0ff","trusted":true},"outputs":[],"source":["# from google.colab import files\n","# files.download('dmi_mifgsm.tgz') "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4N6Me0GQECfZ"},"source":["## Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxNrXHKsEDGx","outputId":"685bc7b1-5241-4f4e-9910-ee59392a1074","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","plt.figure(figsize=(10, 20))\n","cnt = 0\n","for i, cls_name in enumerate(classes):\n","    path = f'{cls_name}/{cls_name}1.png'\n","    # benign image\n","    cnt += 1\n","    plt.subplot(len(classes), 4, cnt)\n","    im = Image.open(f'./data/{path}')\n","    logit = model(transform(im).unsqueeze(0).to(device))[0]\n","    predict = logit.argmax(-1).item()\n","    prob = logit.softmax(-1)[predict].item()\n","    plt.title(f'benign: {cls_name}1.png\\n{classes[predict]}: {prob:.2%}')\n","    plt.axis('off')\n","    plt.imshow(np.array(im))\n","    # adversarial image\n","    cnt += 1\n","    plt.subplot(len(classes), 4, cnt)\n","    im = Image.open(f'./dmi_mifgsm/{path}')\n","    logit = model(transform(im).unsqueeze(0).to(device))[0]\n","    predict = logit.argmax(-1).item()\n","    prob = logit.softmax(-1)[predict].item()\n","    plt.title(f'adversarial: {cls_name}1.png\\n{classes[predict]}: {prob:.2%}')\n","    plt.axis('off')\n","    plt.imshow(np.array(im))\n","plt.tight_layout()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WDc6QllJEHiC"},"source":["## Report Question\n","* Make sure you follow below setup: the source model is \"resnet110_cifar10\", applying the vanilla fgsm attack on `dog2.png`. You can find the perturbed image in `fgsm/dog2.png`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhFVWA6JEH8Z","outputId":"fe559d96-9614-4f20-8925-85802f6c6539","trusted":true},"outputs":[],"source":["# original image\n","path = f'dog/dog2.png'\n","im = Image.open(f'./data/{path}')\n","logit = model(transform(im).unsqueeze(0).to(device))[0]\n","predict = logit.argmax(-1).item()\n","prob = logit.softmax(-1)[predict].item()\n","plt.title(f'benign: dog2.png\\n{classes[predict]}: {prob:.2%}')\n","plt.axis('off')\n","plt.imshow(np.array(im))\n","plt.tight_layout()\n","plt.show()\n","\n","# adversarial image \n","adv_im = Image.open(f'./fgsm/{path}')\n","logit = model(transform(adv_im).unsqueeze(0).to(device))[0]\n","predict = logit.argmax(-1).item()\n","prob = logit.softmax(-1)[predict].item()\n","plt.title(f'adversarial: dog2.png\\n{classes[predict]}: {prob:.2%}')\n","plt.axis('off')\n","plt.imshow(np.array(adv_im))\n","plt.tight_layout()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NfwhnywXEMwZ"},"source":["## Passive Defense - JPEG compression\n","JPEG compression by imgaug package, compression rate set to 70\n","\n","Reference: https://imgaug.readthedocs.io/en/latest/source/api_augmenters_arithmetic.html#imgaug.augmenters.arithmetic.JpegCompression\n","\n","Note: If you haven't implemented the JPEG compression, this module will return an error. Don't worry about this."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# pip install -U scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import imgaug.augmenters as iaa\n","\n","# pre-process image\n","x = transforms.ToTensor()(adv_im)*255\n","x = x.permute(1, 2, 0).numpy()\n","x = x.astype(np.uint8)\n","\n","# TODO: use \"imgaug\" package to perform JPEG compression (compression rate = 70)\n","test_x =  iaa.arithmetic.compress_jpeg(x, compression=70)\n","\n","logit = model(transform(test_x).unsqueeze(0).to(device))[0]\n","predict = logit.argmax(-1).item()\n","prob = logit.softmax(-1)[predict].item()\n","plt.title(f'JPEG adversarial: dog2.png\\n{classes[predict]}: {prob:.2%}')\n","plt.axis('off')\n","\n","\n","plt.imshow(test_x)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
